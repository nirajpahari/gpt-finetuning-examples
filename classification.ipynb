{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Dataset: Category and sentiment classsification dataset extracted from ACOS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 15:12:38.730404: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-11 15:12:38.769630: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 15:12:39.455983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for classification\n",
    "train_path = \"./data/classification/rest16_quad_train.pkl\"\n",
    "dev_path = \"./data/classification/rest16_quad_dev.pkl\"\n",
    "test_path = \"./data/classification/rest16_quad_test.pkl\"\n",
    "\n",
    "train_df = pd.read_pickle(train_path)\n",
    "val_df = pd.read_pickle(dev_path)\n",
    "test_df = pd.read_pickle(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['string', 'acso', 'tokens', 'bio_tags', 'aspects', 'categories',\n",
       "       'sentiments', 'opinions', 'bio_cat_sent_tags', 'aspect', 'category',\n",
       "       'sentiment', 'opinion', 'main_category', 'sub_category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display all the columns in the df\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "      <th>acso</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bio_tags</th>\n",
       "      <th>aspects</th>\n",
       "      <th>categories</th>\n",
       "      <th>sentiments</th>\n",
       "      <th>opinions</th>\n",
       "      <th>bio_cat_sent_tags</th>\n",
       "      <th>aspect</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>opinion</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>judging from previous posts this used to be a ...</td>\n",
       "      <td>[10,11 RESTAURANT#GENERAL 0 13,16]</td>\n",
       "      <td>[judging, from, previous, posts, this, used, t...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B_Aspect, O, O,...</td>\n",
       "      <td>[[10, 11]]</td>\n",
       "      <td>[RESTAURANT#GENERAL]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[13, 16]]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B_RESTAURANT#GE...</td>\n",
       "      <td>[10, 11]</td>\n",
       "      <td>RESTAURANT#GENERAL</td>\n",
       "      <td>0</td>\n",
       "      <td>[13, 16]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we , there were four of us , arrived at noon -...</td>\n",
       "      <td>[19,20 SERVICE#GENERAL 0 31,32]</td>\n",
       "      <td>[we, ,, there, were, four, of, us, ,, arrived,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[[19, 20]]</td>\n",
       "      <td>[SERVICE#GENERAL]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[31, 32]]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[19, 20]</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>0</td>\n",
       "      <td>[31, 32]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they never brought us complimentary noodles , ...</td>\n",
       "      <td>[-1,-1 SERVICE#GENERAL 0 -1,-1]</td>\n",
       "      <td>[they, never, brought, us, complimentary, nood...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[[-1, -1]]</td>\n",
       "      <td>[SERVICE#GENERAL]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[-1, -1]]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>after all that , they complained to me about t...</td>\n",
       "      <td>[-1,-1 SERVICE#GENERAL 0 5,6]</td>\n",
       "      <td>[after, all, that, ,, they, complained, to, me...</td>\n",
       "      <td>[O, O, O, O, O, B_Opinion, O, O, O, O, O, O, B...</td>\n",
       "      <td>[[-1, -1]]</td>\n",
       "      <td>[SERVICE#GENERAL]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "      <td>[O, O, O, O, O, B_Negative, O, O, O, O, O, O, ...</td>\n",
       "      <td>[-1, -1]</td>\n",
       "      <td>SERVICE#GENERAL</td>\n",
       "      <td>0</td>\n",
       "      <td>[5, 6]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avoid this place !</td>\n",
       "      <td>[2,3 RESTAURANT#GENERAL 0 0,1]</td>\n",
       "      <td>[avoid, this, place, !]</td>\n",
       "      <td>[B_Opinion, O, B_Aspect, O]</td>\n",
       "      <td>[[2, 3]]</td>\n",
       "      <td>[RESTAURANT#GENERAL]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[0, 1]]</td>\n",
       "      <td>[B_Negative, O, B_RESTAURANT#GENERAL, O]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>RESTAURANT#GENERAL</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              string  \\\n",
       "0  judging from previous posts this used to be a ...   \n",
       "1  we , there were four of us , arrived at noon -...   \n",
       "2  they never brought us complimentary noodles , ...   \n",
       "3  after all that , they complained to me about t...   \n",
       "4                                 avoid this place !   \n",
       "\n",
       "                                 acso  \\\n",
       "0  [10,11 RESTAURANT#GENERAL 0 13,16]   \n",
       "1     [19,20 SERVICE#GENERAL 0 31,32]   \n",
       "2     [-1,-1 SERVICE#GENERAL 0 -1,-1]   \n",
       "3       [-1,-1 SERVICE#GENERAL 0 5,6]   \n",
       "4      [2,3 RESTAURANT#GENERAL 0 0,1]   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [judging, from, previous, posts, this, used, t...   \n",
       "1  [we, ,, there, were, four, of, us, ,, arrived,...   \n",
       "2  [they, never, brought, us, complimentary, nood...   \n",
       "3  [after, all, that, ,, they, complained, to, me...   \n",
       "4                            [avoid, this, place, !]   \n",
       "\n",
       "                                            bio_tags     aspects  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, B_Aspect, O, O,...  [[10, 11]]   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [[19, 20]]   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [[-1, -1]]   \n",
       "3  [O, O, O, O, O, B_Opinion, O, O, O, O, O, O, B...  [[-1, -1]]   \n",
       "4                        [B_Opinion, O, B_Aspect, O]    [[2, 3]]   \n",
       "\n",
       "             categories sentiments    opinions  \\\n",
       "0  [RESTAURANT#GENERAL]        [0]  [[13, 16]]   \n",
       "1     [SERVICE#GENERAL]        [0]  [[31, 32]]   \n",
       "2     [SERVICE#GENERAL]        [0]  [[-1, -1]]   \n",
       "3     [SERVICE#GENERAL]        [0]    [[5, 6]]   \n",
       "4  [RESTAURANT#GENERAL]        [0]    [[0, 1]]   \n",
       "\n",
       "                                   bio_cat_sent_tags    aspect  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, B_RESTAURANT#GE...  [10, 11]   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [19, 20]   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [-1, -1]   \n",
       "3  [O, O, O, O, O, B_Negative, O, O, O, O, O, O, ...  [-1, -1]   \n",
       "4           [B_Negative, O, B_RESTAURANT#GENERAL, O]    [2, 3]   \n",
       "\n",
       "             category sentiment   opinion main_category sub_category  \n",
       "0  RESTAURANT#GENERAL         0  [13, 16]           NaN          NaN  \n",
       "1     SERVICE#GENERAL         0  [31, 32]           NaN          NaN  \n",
       "2     SERVICE#GENERAL         0  [-1, -1]           NaN          NaN  \n",
       "3     SERVICE#GENERAL         0    [5, 6]           NaN          NaN  \n",
       "4  RESTAURANT#GENERAL         0    [0, 1]           NaN          NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label dictionaries for categories\n",
    "label2id = {label: i for i, label in enumerate(train_df['category'].unique())}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RESTAURANT#GENERAL': 0, 'SERVICE#GENERAL': 1, 'FOOD#QUALITY': 2, 'RESTAURANT#MISCELLANEOUS': 3, 'FOOD#STYLE_OPTIONS': 4, 'AMBIENCE#GENERAL': 5, 'LOCATION#GENERAL': 6, 'DRINKS#QUALITY': 7, 'FOOD#PRICES': 8, 'RESTAURANT#PRICES': 9, 'DRINKS#STYLE_OPTIONS': 10, 'DRINKS#PRICES': 11}\n"
     ]
    }
   ],
   "source": [
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and labels for category and sentiment tasks\n",
    "train_sents = train_df['string'].tolist()\n",
    "train_cat_labels = train_df['category'].map(label2id).tolist()\n",
    "train_sent_labels = train_df['sentiment'].tolist()\n",
    "train_sent_labels = [int(i) for i in train_sent_labels]\n",
    "\n",
    "val_sents = val_df['string'].tolist()\n",
    "val_cat_labels = val_df['category'].map(label2id).tolist()\n",
    "val_sent_labels = val_df['sentiment'].tolist()\n",
    "val_sent_labels = [int(i) for i in val_sent_labels]\n",
    "\n",
    "test_sents = test_df['string'].tolist()\n",
    "test_cat_labels = test_df['category'].map(label2id).tolist()\n",
    "test_sent_labels = test_df['sentiment'].tolist()\n",
    "test_sent_labels = [int(i) for i in test_sent_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning function\n",
    "def fine_tune_gpt2(train_dataset, val_dataset, test_dataset, model_name, output_dir,num_labels, epochs=3):\n",
    "    # Load pre-trained GPT-2 model with a sequence classification head\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    \n",
    "    # Set up training parameters\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = 'cpu'\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "    # Prepare data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    # Fine-tuning loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_labels, val_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}, Validation Accuracy: {val_accuracy}, Validataion F1: {val_f1}\")\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_labels, test_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_preds.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
    "\n",
    "    test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "    test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "    print(f\"Testing Accuracy: {test_accuracy}\")\n",
    "    print(f\"Testing F1: {test_f1}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to customdataset class\n",
    "\n",
    "cat_train_dataset = CustomDataset(train_sents, train_cat_labels, tokenizer, max_length=128)\n",
    "cat_val_dataset = CustomDataset(val_sents, val_cat_labels, tokenizer, max_length=128)\n",
    "cat_test_dataset = CustomDataset(test_sents, test_cat_labels, tokenizer, max_length=128)\n",
    "\n",
    "sent_train_dataset = CustomDataset(train_sents, train_sent_labels, tokenizer, max_length=128)\n",
    "sent_val_dataset = CustomDataset(val_sents, val_sent_labels, tokenizer, max_length=128)\n",
    "sent_test_dataset = CustomDataset(test_sents, test_sent_labels, tokenizer, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/hop/home/pahari_niraj/Projects/.env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.7819997072219849, Validation Accuracy: 0.29245283018867924, Validataion F1: 0.08651347977190674\n",
      "Epoch 2/5, Loss: 2.3995401859283447, Validation Accuracy: 0.5660377358490566, Validataion F1: 0.19522101002229583\n",
      "Epoch 3/5, Loss: 1.1191128492355347, Validation Accuracy: 0.5849056603773585, Validataion F1: 0.21050471547983987\n",
      "Epoch 4/5, Loss: 1.976608395576477, Validation Accuracy: 0.7452830188679245, Validataion F1: 0.3806674806674807\n",
      "Epoch 5/5, Loss: 0.7307218313217163, Validation Accuracy: 0.7452830188679245, Validataion F1: 0.4691807702074214\n",
      "Testing Accuracy: 0.6351351351351351\n",
      "Testing F1: 0.24950589381639224\n"
     ]
    }
   ],
   "source": [
    "# Finetuning for category classification\n",
    "fine_tune_gpt2(cat_train_dataset, cat_val_dataset, cat_test_dataset, model_name, './output/category/',num_labels=len(label2id), epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/hop/home/pahari_niraj/Projects/.env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8016752004623413, Validation Accuracy: 0.8113207547169812, Validataion F1: 0.4986292533462344\n",
      "Epoch 2/5, Loss: 0.9988476037979126, Validation Accuracy: 0.7641509433962265, Validataion F1: 0.4349482852476864\n",
      "Epoch 3/5, Loss: 0.25896257162094116, Validation Accuracy: 0.8490566037735849, Validataion F1: 0.5493197278911565\n",
      "Epoch 4/5, Loss: 0.542195200920105, Validation Accuracy: 0.8584905660377359, Validataion F1: 0.557780119423955\n",
      "Epoch 5/5, Loss: 0.15622447431087494, Validation Accuracy: 0.839622641509434, Validataion F1: 0.5326194273562694\n",
      "Testing Accuracy: 0.7702702702702703\n",
      "Testing F1: 0.5532072949805585\n"
     ]
    }
   ],
   "source": [
    "fine_tune_gpt2(sent_train_dataset, sent_val_dataset, sent_test_dataset, model_name, './output/sentiment/', num_labels=3, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
